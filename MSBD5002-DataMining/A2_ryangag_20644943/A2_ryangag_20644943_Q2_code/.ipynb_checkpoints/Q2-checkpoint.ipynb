{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 Implementation of Adaboost\n",
    "\n",
    "The following table shows the training dataset. It consists of 10 data and 2 labels.\n",
    "\n",
    "|  |    |    |     |   |    |    |    |    |    |    |\n",
    "|--|----|----|-----|---|----|----|----|----|----|----|\n",
    "|x |0   |1   |2    |3  |4   |5   |6   |7   |8   | 9  |\n",
    "|y |1   |1   |1    |-1 |-1  |-1  |1   |1   |1   |-1  |\n",
    "\n",
    "\n",
    "We assume the weak classifier is produced by $x < v$ or $x > v$ where $v$ is the threshold and makes the classifier get the best accuracy on the dataset. You should implement the **AdaBoost** algorithm to learn a **strong classifier**. Notice that you CANNOT use Adaboost library. You need to implement it manually.\n",
    "\n",
    "You should also report the final expression of the strong classifier, such as $C^âˆ—(x) = sign [\\alpha_1 C_1(x) + \\alpha_2 C_2(x) + \\alpha_3 C_3(x) + \\cdots]$, where $C_i(x)$ is the base classifier and $\\alpha_i$ is the weight of base classifier. You are also required to describe each basic classifier in detail.\n",
    "\n",
    "For simplicity, the threshold $v$ should be the multiple of 0.5, i.e., $v\\%0.5==0$. For example, you can set $v$ as 2, 2.5, or 3, but you cannot set $v$ as 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class BaseClassifier:\n",
    "    \"\"\"\n",
    "    The Base Classifier Class\n",
    "    \n",
    "    Initial Parameters:\n",
    "        v: the threshold of the base classifier\n",
    "        name: (string) the name of the base classifier\n",
    "        lower_v_label: (+1 or -1) the label when x < v\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, v, name, lower_v_label):\n",
    "        self.v = v\n",
    "        self.name = name\n",
    "        self.lower_v_label = lower_v_label\n",
    "        \n",
    "    def predict(self, X):\n",
    "        y_predict = list()\n",
    "        for x in X:\n",
    "            if x < self.v:\n",
    "                y_predict.append(self.lower_v_label)\n",
    "            else:\n",
    "                y_predict.append(-self.lower_v_label)\n",
    "        return y_predict\n",
    "        \n",
    "    def get_error_rate(self, X, y, weights):\n",
    "        weight_count = 0\n",
    "        total = len(y)\n",
    "        y_predict = self.predict(X)\n",
    "        for i in range(total):\n",
    "            if y_predict[i] != y[i]:\n",
    "                weight_count += weights[i]\n",
    "        return weight_count / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider from two sides to find all best base classifiers, firstly we consider this situation,\n",
    "\n",
    "$$\n",
    "C(x) = \\left\\{\n",
    "\\begin{aligned}\n",
    "+1 &  , x < v \\\\\n",
    "-1 & , \\ x \\geq v\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "With $v$ satisfying $v\\%0.5==0$, it's obvious when $v=2.5$ or $v=8.5$, classifier achieves the lowest error rate 0.3, only misclassfying 3 samples. We set the two classifiers as $C_1$ and $C_2$:\n",
    "\n",
    "$$\n",
    "C_1(x) = \\left\\{\n",
    "\\begin{aligned}\n",
    "+1 &  , x < 2.5 \\\\\n",
    "-1 & , \\ x \\geq 2.5\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\ \\ \\ \\ \\\n",
    "C_2(x) = \\left\\{\n",
    "\\begin{aligned}\n",
    "+1 &  , x < 8.5 \\\\\n",
    "-1 & , \\ x \\geq 8.5\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Then we consider a classifier with this form,\n",
    "\n",
    "$$\n",
    "C(x) = \\left\\{\n",
    "\\begin{aligned}\n",
    "-1 &  , x < v \\\\\n",
    "+1 & , \\ x \\geq v\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Similarly, we can find that the classifier with $v=5.5$ has the smallest error rate of 0.4. We denote it by $C_3$:\n",
    "\n",
    "$$\n",
    "C_3(x) = \\left\\{\n",
    "\\begin{aligned}\n",
    "-1 &  , x < 5.5 \\\\\n",
    "+1 & , \\ x \\geq 5.5\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier 1: v < 2.5 => y = +1, v >= 2.5 => y = -1\n",
      "prediction: [1, 1, 1, -1, -1, -1, -1, -1, -1, -1]\n",
      "error rate:  0.3\n",
      "\n",
      "Classifier 1: v < 8.5 => y = +1, v >= 8.5 => y = -1\n",
      "prediction: [1, 1, 1, 1, 1, 1, 1, 1, 1, -1]\n",
      "error rate:  0.3\n",
      "\n",
      "Classifier 1: v < 2.5 => y = -1, v >= 2.5 => y = +1\n",
      "prediction: [-1, -1, -1, -1, -1, -1, 1, 1, 1, 1]\n",
      "error rate:  0.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Dataset\n",
    "X = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "y = [1, 1, 1, -1, -1, -1, 1, 1, 1, -1]\n",
    "\n",
    "weights = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Initialize three best basic classifiers C1, C2, C3\n",
    "# v = 2.5, lower_v_label = 1\n",
    "C1 = BaseClassifier(v=2.5, name=\"C1\", lower_v_label=1)\n",
    "print(\"Classifier 1: v < 2.5 => y = +1, v >= 2.5 => y = -1\")\n",
    "print(\"prediction:\",C1.predict(X))\n",
    "print(\"error rate: \",C1.get_error_rate(X, y, weights))\n",
    "print()\n",
    "\n",
    "# v = 8.5, lower_v_label = 1\n",
    "C2 = BaseClassifier(v=8.5, name=\"C2\", lower_v_label=1)\n",
    "print(\"Classifier 1: v < 8.5 => y = +1, v >= 8.5 => y = -1\")\n",
    "print(\"prediction:\", C2.predict(X))\n",
    "print(\"error rate: \", C2.get_error_rate(X, y, weights))\n",
    "print()\n",
    "\n",
    "# v = 5.5, lower_v_label = -1\n",
    "C3 = BaseClassifier(v=5.5, name=\"C3\", lower_v_label=-1)\n",
    "print(\"Classifier 1: v < 2.5 => y = -1, v >= 2.5 => y = +1\")\n",
    "print(\"prediction:\",C3.predict(X))\n",
    "print(\"error rate: \",C3.get_error_rate(X, y, weights))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaboost:\n",
    "    \"\"\"\n",
    "    Adaboost Classifier Class\n",
    "    \n",
    "    Initial Parameters:\n",
    "        base_classifiers: [Classifier1, Classifier2, ...] the list of base classifiers used to train\n",
    "        n_classifiers: The maximum number of classifiers at which boosting is terminated\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_classifiers, n_classifiers):\n",
    "        self.base_classifiers = base_classifiers\n",
    "        self.n_classifiers = n_classifiers\n",
    "    \n",
    "    # train on data X -> y\n",
    "    def fit(self, X, y):\n",
    "        weights = [1/len(X)] * len(X) # initialize weight\n",
    "        self.alphas = list()\n",
    "        self.classifiers = list()\n",
    "        for n in range(self.n_classifiers):\n",
    "            min_error_rate = 1\n",
    "            for cf in self.base_classifiers:\n",
    "                e = cf.get_error_rate(X, y, weights)\n",
    "                print(cf.name, \"error rate = \", e)\n",
    "                if e < min_error_rate:\n",
    "                    best_base_classifier = cf\n",
    "                    min_error_rate = e\n",
    "            \n",
    "            # calculate the importance of the classifier\n",
    "            alpha = 1/2 * math.log((1-min_error_rate)/min_error_rate) \n",
    "            \n",
    "            # save alpha and the best_base_classifier at each iteration\n",
    "            self.alphas.append(alpha)\n",
    "            self.classifiers.append(best_base_classifier)\n",
    "            \n",
    "            # update weights\n",
    "            y_predict = best_base_classifier.predict(X)\n",
    "            for i in range(len(weights)):\n",
    "                if y_predict[i] == y[i]:\n",
    "                    weights[i] = weights[i] / (2 * (1 - min_error_rate))\n",
    "                else:\n",
    "                    weights[i] = weights[i] / (2 * min_error_rate)\n",
    "            print(\"weights = \", weights)\n",
    "            print()\n",
    "            \n",
    "        # the information to print to describe the final ensemble classifier\n",
    "        info = \"final ensemble classifier = \"\n",
    "        i = 0\n",
    "        for i in range(len(self.alphas)):\n",
    "            if i < len(self.alphas) - 1:\n",
    "                info += (str(self.alphas[i]) + \" * \" + self.classifiers[i].name + \" + \")\n",
    "            else:\n",
    "                info += (str(self.alphas[i]) + \" * \" + self.classifiers[i].name)\n",
    "        print(info)\n",
    "\n",
    "    def predict(self, X):\n",
    "        scores = np.array([0.0]*len(X))\n",
    "        for i in range(len(self.alphas)):\n",
    "            scores += self.alphas[i] * np.array(self.classifiers[i].predict(X))\n",
    "        \n",
    "        return np.sign(scores)\n",
    "    \n",
    "    def get_error_rate(self, X, y, weights):\n",
    "        weight_count = 0\n",
    "        total = len(y)\n",
    "        y_predict = self.predict(X)\n",
    "        for i in range(total):\n",
    "            if y_predict[i] != y[i]:\n",
    "                weight_count += weights[i]\n",
    "        return weight_count / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1 error rate =  0.030000000000000006\n",
      "C2 error rate =  0.030000000000000006\n",
      "C3 error rate =  0.04\n",
      "weights =  [0.051546391752577324, 0.051546391752577324, 0.051546391752577324, 0.051546391752577324, 0.051546391752577324, 0.051546391752577324, 1.6666666666666665, 1.6666666666666665, 1.6666666666666665, 0.051546391752577324]\n",
      "\n",
      "C1 error rate =  0.5\n",
      "C2 error rate =  0.015463917525773196\n",
      "C3 error rate =  0.02061855670103093\n",
      "weights =  [0.02617801047120419, 0.02617801047120419, 0.02617801047120419, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 0.8464223385689353, 0.8464223385689353, 0.8464223385689353, 0.02617801047120419]\n",
      "\n",
      "C1 error rate =  0.25392670157068065\n",
      "C2 error rate =  0.5\n",
      "C3 error rate =  0.010471204188481676\n",
      "weights =  [1.25, 1.25, 1.25, 0.8421516754850089, 0.8421516754850089, 0.8421516754850089, 0.42768959435626097, 0.42768959435626097, 0.42768959435626097, 1.25]\n",
      "\n",
      "final ensemble classifier = 1.7380493449176364 * C1 + 2.07683056968926 * C2 + 2.2742999172498486 * C3\n"
     ]
    }
   ],
   "source": [
    "ada = Adaboost(base_classifiers=[C1, C2, C3], n_classifiers=3)\n",
    "ada.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada.get_error_rate(X, y, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we get a strong classifier $C^*(x)$\n",
    "\n",
    "$$C^*(x) = sign(1.7380493449176364 * C_1(x) + 2.07683056968926 * C_2(x) + 2.2742999172498486 * C_3(x))$$\n",
    "\n",
    "that can achieve **0 error of classification** where\n",
    "\n",
    "$$\n",
    "C_1(x) = \\left\\{\n",
    "\\begin{aligned}\n",
    "+1 &  , x < 2.5 \\\\\n",
    "-1 & , \\ x \\geq 2.5\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\ \\ \\ \\ \\\n",
    "C_2(x) = \\left\\{\n",
    "\\begin{aligned}\n",
    "+1 &  , x < 8.5 \\\\\n",
    "-1 & , \\ x \\geq 8.5\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\ \\ \\ \\ \\ \n",
    "C_3(x) = \\left\\{\n",
    "\\begin{aligned}\n",
    "-1 &  , x < 5.5 \\\\\n",
    "+1 & , \\ x \\geq 5.5\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "The classification result of $C^*(x)$ is `[1, 1, 1, -1, -1, -1, 1, 1, 1, -1]`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
