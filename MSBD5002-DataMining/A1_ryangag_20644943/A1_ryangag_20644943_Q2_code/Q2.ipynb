{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. FP-Tree\n",
    "\n",
    "Suppose you are given some transactions and a vocabulary that map terms to indexes. Please use\n",
    "FP-Tree algorithm to discover the frequent itemsets.\n",
    "\n",
    "\n",
    "## Data Description:\n",
    "\n",
    "* topi-i.txt\n",
    "\n",
    "Input file of frequent pattern mining algorithms. Each line represents a transaction with indices of terms.\n",
    "\n",
    "format: term1_index term2_index term3_index ...\n",
    "\n",
    "Columns are separated by a space.\n",
    "\n",
    "* vocab.txt\n",
    "\n",
    "Dictionary that maps term index to term.\n",
    "\n",
    "format: term_index term\n",
    "\n",
    "Columns are separated by a space.\n",
    "\n",
    "* pattern-i.txt:\n",
    "\n",
    "The file you need to submit, which contains your result for this frequent pattern mining task. Each line represents a transaction with frequent itemsets sorted in descending order of support count.\n",
    "\n",
    "format: support_count term1 term2 ...\n",
    "\n",
    "support_count and term1 are separated by a tab, while terms are separated by a space.\n",
    "\n",
    "Here we give an example:\n",
    "```\n",
    "233 rule association\n",
    "230 random\n",
    "227 finding\n",
    "203 mining pattern\n",
    "```\n",
    "## Questions:\n",
    "\n",
    "(a) Please write a program to implement FP-growth algorithm and find all frequent itemsets with `support >= 400` in the given dataset.\n",
    "\n",
    "(b) Based on the result of (a), please print out those FP-conditional trees whose height is **larger than 1**.\n",
    "\n",
    "![Q2-output](Q2-output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define preprocessed funtion\n",
    "\n",
    "* `read_file()` function that converts txt into nested list\n",
    "* `create_init_set()` function that generates transaction dictionary which is as initial data to input `create_fp_tree()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    \"\"\"\n",
    "    Read files from txt \n",
    "    \"\"\"\n",
    "    items_bought = [] # origin data\n",
    "    with open(file,\"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            items_bought.append(line.strip().split())\n",
    "    return items_bought\n",
    "\n",
    "def create_init_set(items_bought):\n",
    "    trans_dict={}\n",
    "    for trans in items_bought:\n",
    "        key = frozenset(trans)\n",
    "        if key not in trans_dict:\n",
    "            trans_dict[key] = 1\n",
    "        else:\n",
    "            trans_dict[key] += 1\n",
    "    return trans_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the class of TreeNode and make a vocab dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vocab_dict which is used to transform index to vocabulary\n",
    "vocab_dict = {}\n",
    "with open(\"vocab.txt\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        term = line.strip().split(\"\\t\")\n",
    "        vocab_dict[term[0]] = term[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the structure of TreeNode\n",
    "class TreeNode:\n",
    "    def __init__(self, name_value, num_occur, parent_node):\n",
    "        self.name = name_value\n",
    "        self.node_link = None\n",
    "        self.count = num_occur\n",
    "        self.parent = parent_node\n",
    "        self.children = {}\n",
    "    \n",
    "    # Add the count of node\n",
    "    def inc(self, num_occur):\n",
    "        self.count += num_occur\n",
    "    \n",
    "    # Print tree's structrue in the terminal\n",
    "    def display(self, index=1):\n",
    "        print('  '*index, self.name, ' ', self.count)\n",
    "        for child in self.children.values():\n",
    "            child.display(index+1)\n",
    "    \n",
    "    # Calculate the height of tree\n",
    "    def get_height(self, index=1):\n",
    "        mid = 0\n",
    "        for child in self.children.values():\n",
    "            if child.get_height(index+1) >= mid:\n",
    "                mid = child.get_height(index+1)\n",
    "        return max(index, mid)\n",
    "    \n",
    "    # Transform tree to a nested list\n",
    "    # According to the output EXAMPLE => [parent, children]\n",
    "    def transform_to_list(self):\n",
    "        if self.name in vocab_dict:\n",
    "            node_info = vocab_dict[self.name] + \"    \" + str(self.count)\n",
    "        else:\n",
    "            node_info = self.name + \"    \" + str(self.count)\n",
    "        if len(self.children) == 0:\n",
    "            return node_info\n",
    "        \n",
    "        local_list = []\n",
    "        for child in self.children.values():\n",
    "            local_list.append(child.transform_to_list())\n",
    "\n",
    "        return [node_info, local_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define FP-growth function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_header(node_to_test, target_node):\n",
    "    \"\"\"\n",
    "    Update header table\n",
    "    \n",
    "    :node_to_test: the entry of the node\n",
    "    :target_node: Target node to link\n",
    "    \"\"\"\n",
    "    # To find the node with node_link == None\n",
    "    while node_to_test.node_link != None:\n",
    "        node_to_test = node_to_test.node_link\n",
    "    # Link the target_node on node\n",
    "    node_to_test.node_link = target_node\n",
    "            \n",
    "def update_fp_tree(items, fp_tree, header_table, count):\n",
    "    \"\"\"\n",
    "    Update FP Tree\n",
    "    \n",
    "    :items: items in frequent itemsets\n",
    "    :fp_tree: TreeNode\n",
    "    :header_table: header table\n",
    "    :count: added count\n",
    "    \"\"\"\n",
    "    if items[0] in fp_tree.children:\n",
    "        # If items[0] is as child node, fp_tree.count += count\n",
    "        fp_tree.children[items[0]].inc(count)\n",
    "    else:\n",
    "        # If items[0] is not a child node, create a new branch\n",
    "        fp_tree.children[items[0]] = TreeNode(items[0], count, fp_tree)\n",
    "        # Update linked list of the frequent itemsets\n",
    "        if header_table[items[0]][1] == None:\n",
    "            header_table[items[0]][1] = fp_tree.children[items[0]]\n",
    "        else:\n",
    "            update_header(header_table[items[0]][1], fp_tree.children[items[0]])\n",
    "    # Recursion\n",
    "    if len(items) > 1:\n",
    "        update_fp_tree(items[1::], fp_tree.children[items[0]], header_table, count)\n",
    "            \n",
    "def create_fp_tree(trans_dict, threshold=400):\n",
    "    \"\"\"\n",
    "    Create FP Tree\n",
    "    \n",
    "    :trans_dict: transaction dictionary\n",
    "    :threshold: bound that decides whether to remove the item in transactions. Default value = 400\n",
    "    \n",
    "    :return: fp_tree, header_table, filtered_trans_dict\n",
    "    \"\"\"\n",
    "    header_table = {}\n",
    "    \n",
    "    # First Scan: Construct the header table and sort it\n",
    "    for items in trans_dict:\n",
    "        for item in items:\n",
    "            header_table[item] = header_table.get(item, 0) + trans_dict[items]\n",
    "    # Delete the elements with frequency < threshold\n",
    "    for k in set(header_table.keys()):\n",
    "        if header_table[k] < threshold:\n",
    "            del(header_table[k]) \n",
    "    # Sort the header table by descending frequency\n",
    "    header_table = dict(sorted(header_table.items(), key=lambda p:p[1], reverse=True))\n",
    "    # Frequent itemsets contain items whose frequency >= threshold\n",
    "    freq_itemsets = set(header_table.keys())\n",
    "    \n",
    "    if len(freq_itemsets) == 0:\n",
    "        return None, None, None\n",
    "    \n",
    "    for k in header_table:\n",
    "        header_table[k] = [header_table[k], None]\n",
    "\n",
    "    # Initialize the FP Tree with root node\n",
    "    fp_tree = TreeNode('Null Set', 1, None)\n",
    "    \n",
    "    # Second Scan: remove the infrequent item in transaction and sort it\n",
    "    filtered_trans_dict = {}\n",
    "    for trans, count in trans_dict.items():\n",
    "        local_dict = {}\n",
    "        for item in trans:\n",
    "            if item in freq_itemsets:\n",
    "                local_dict[item] = header_table[item][0]\n",
    "        if len(local_dict) > 0:\n",
    "            # Sorted transaction item by descending frequency\n",
    "            ordered_items = [x[0] for x in sorted(local_dict.items(), key=lambda p:p[1], reverse=True)]\n",
    "            \n",
    "            # Build the filtered transaction dictionary\n",
    "            key = frozenset(ordered_items)\n",
    "            if key not in filtered_trans_dict:\n",
    "                filtered_trans_dict[key] = count\n",
    "            else:\n",
    "                filtered_trans_dict[key] += count\n",
    "\n",
    "            # Update FP Tree using ordered items\n",
    "            update_fp_tree(ordered_items, fp_tree, header_table, count)\n",
    "    \n",
    "    # Sort the filtered transaction dictionary as an output\n",
    "    filtered_trans_dict = dict(sorted(filtered_trans_dict.items(), key=lambda p:p[1], reverse=True))\n",
    "    \n",
    "    return fp_tree, header_table, filtered_trans_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate FP-Tree from txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{frozenset({'248'}): 668, frozenset({'390'}): 624, frozenset({'458'}): 500, frozenset({'298'}): 328, frozenset({'382'}): 324, frozenset({'118'}): 303, frozenset({'382', '390'}): 296, frozenset({'473'}): 217, frozenset({'723'}): 208, frozenset({'225'}): 170, frozenset({'382', '723'}): 123, frozenset({'382', '225'}): 108, frozenset({'473', '248'}): 67, frozenset({'458', '248'}): 52, frozenset({'298', '248'}): 47, frozenset({'248', '390'}): 45, frozenset({'298', '390'}): 38, frozenset({'473', '390'}): 37, frozenset({'248', '118'}): 37, frozenset({'382', '458'}): 33, frozenset({'382', '248'}): 28, frozenset({'118', '390'}): 26, frozenset({'225', '248'}): 23, frozenset({'458', '390'}): 23, frozenset({'225', '390'}): 23, frozenset({'723', '390'}): 23, frozenset({'382', '723', '390'}): 23, frozenset({'298', '458'}): 21, frozenset({'382', '248', '390'}): 20, frozenset({'382', '298', '390'}): 20, frozenset({'723', '118'}): 20, frozenset({'298', '723'}): 18, frozenset({'458', '118'}): 18, frozenset({'382', '473'}): 18, frozenset({'382', '225', '390'}): 15, frozenset({'382', '225', '248'}): 14, frozenset({'473', '118'}): 13, frozenset({'298', '118'}): 13, frozenset({'382', '473', '723'}): 12, frozenset({'382', '118', '390'}): 12, frozenset({'473', '723'}): 11, frozenset({'225', '473'}): 11, frozenset({'382', '458', '723'}): 11, frozenset({'382', '298'}): 11, frozenset({'298', '473'}): 9, frozenset({'298', '458', '248'}): 9, frozenset({'458', '473'}): 9, frozenset({'382', '473', '390'}): 9, frozenset({'298', '225'}): 9, frozenset({'382', '118'}): 9, frozenset({'458', '723'}): 9, frozenset({'382', '225', '473'}): 8, frozenset({'473', '248', '118'}): 8, frozenset({'382', '723', '118'}): 7, frozenset({'723', '248'}): 7, frozenset({'382', '723', '248'}): 7, frozenset({'458', '473', '248'}): 7, frozenset({'473', '248', '390'}): 6, frozenset({'382', '723', '298'}): 6, frozenset({'382', '473', '248'}): 5, frozenset({'225', '118'}): 5, frozenset({'118', '723', '390'}): 4, frozenset({'298', '473', '390'}): 4, frozenset({'382', '458', '390'}): 4, frozenset({'225', '723'}): 4, frozenset({'382', '225', '248', '390'}): 4, frozenset({'118', '473', '390'}): 3, frozenset({'382', '458', '118'}): 3, frozenset({'382', '723', '298', '390'}): 3, frozenset({'382', '225', '118'}): 3, frozenset({'298', '458', '473'}): 2, frozenset({'118', '248', '390'}): 2, frozenset({'458', '723', '248', '473'}): 2, frozenset({'382', '225', '298'}): 2, frozenset({'382', '473', '118'}): 2, frozenset({'225', '248', '390'}): 2, frozenset({'458', '723', '390'}): 2, frozenset({'382', '473', '723', '118'}): 2, frozenset({'382', '225', '473', '248'}): 2, frozenset({'382', '473', '723', '390'}): 2, frozenset({'298', '458', '118'}): 2, frozenset({'382', '225', '473', '390'}): 1, frozenset({'298', '473', '723'}): 1, frozenset({'225', '473', '723', '390'}): 1, frozenset({'458', '225'}): 1, frozenset({'382', '248', '118'}): 1, frozenset({'458', '723', '118'}): 1, frozenset({'298', '473', '118'}): 1, frozenset({'298', '118', '390'}): 1, frozenset({'382', '473', '723', '298'}): 1, frozenset({'723', '248', '118'}): 1, frozenset({'298', '723', '390'}): 1, frozenset({'382', '248', '298'}): 1, frozenset({'225', '473', '248'}): 1, frozenset({'382', '225', '723'}): 1, frozenset({'382', '458', '298'}): 1, frozenset({'382', '723', '248', '473'}): 1, frozenset({'382', '225', '723', '298'}): 1, frozenset({'382', '473', '298'}): 1, frozenset({'298', '248', '390'}): 1, frozenset({'225', '248', '118'}): 1, frozenset({'298', '723', '248', '118'}): 1, frozenset({'458', '225', '248'}): 1, frozenset({'723', '248', '390'}): 1, frozenset({'118', '458', '390'}): 1, frozenset({'118', '225', '390'}): 1, frozenset({'723', '390', '473', '382', '248'}): 1, frozenset({'382', '458', '473', '723'}): 1, frozenset({'382', '118', '723', '390'}): 1, frozenset({'298', '225', '723'}): 1, frozenset({'382', '458', '248', '390'}): 1, frozenset({'225', '723', '248'}): 1, frozenset({'473', '723', '390'}): 1, frozenset({'298', '473', '248', '390'}): 1, frozenset({'118', '473', '723', '390'}): 1, frozenset({'458', '248', '118'}): 1, frozenset({'723', '248', '473', '118'}): 1, frozenset({'382', '473', '248', '390'}): 1, frozenset({'473', '723', '118'}): 1, frozenset({'225', '473', '118'}): 1, frozenset({'298', '248', '118'}): 1, frozenset({'458', '225', '390'}): 1, frozenset({'382', '458', '248'}): 1, frozenset({'458', '248', '390'}): 1, frozenset({'298', '723', '248'}): 1, frozenset({'382', '458', '473'}): 1, frozenset({'382', '458', '473', '248'}): 1, frozenset({'723', '248', '473'}): 1, frozenset({'298', '723', '118'}): 1, frozenset({'298', '473', '248'}): 1, frozenset({'723', '390', '473', '298', '248'}): 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'390': [1288, <__main__.TreeNode at 0x24a69a5d470>],\n",
       " '382': [1163, <__main__.TreeNode at 0x24a69a5dcf8>],\n",
       " '248': [1087, <__main__.TreeNode at 0x24a69a5dd68>],\n",
       " '458': [720, <__main__.TreeNode at 0x24a69a5d518>],\n",
       " '298': [560, <__main__.TreeNode at 0x24a69a5dc88>],\n",
       " '723': [528, <__main__.TreeNode at 0x24a69a5d6a0>],\n",
       " '118': [509, <__main__.TreeNode at 0x24a69a5dcc0>],\n",
       " '473': [488, <__main__.TreeNode at 0x24a69a5d630>],\n",
       " '225': [416, <__main__.TreeNode at 0x24a69a5db00>]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test for topic-0.txt\n",
    "items_bought = read_file(\"topic-0.txt\")\n",
    "trans_dict = create_init_set(items_bought)\n",
    "fp_tree, header_table, filtered_trans_dict = create_fp_tree(trans_dict, threshold=400)\n",
    "\n",
    "# fp_tree.display()\n",
    "print(filtered_trans_dict)\n",
    "header_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin to Mine FP-tree\n",
    "\n",
    "To mine the FP-tree, we need to start from certain node and go through all nodes in the direction of root and build conditional pattern base. Then using the pattern base to generate FP-conditional tree. Finally, through recursively mining FP-conditional tree, we can gain the frequent itemsets pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ascend FP Tree\n",
    "def ascend_fp_tree(leaf_node, prefix_path):\n",
    "    if leaf_node.parent != None:\n",
    "        prefix_path.append(leaf_node.name)\n",
    "        ascend_fp_tree(leaf_node.parent, prefix_path)\n",
    "        \n",
    "# Find FP-conditional base\n",
    "def find_cond_pat_base(base_pat, header_table):\n",
    "    tree_node = header_table[base_pat][1]\n",
    "    cond_pats = {}\n",
    "    while tree_node != None:\n",
    "        prefix_path = []\n",
    "        ascend_fp_tree(tree_node, prefix_path)\n",
    "        if len(prefix_path) > 1:\n",
    "            cond_pats[frozenset(prefix_path[1:])] = tree_node.count\n",
    "        tree_node = tree_node.node_link\n",
    "    return cond_pats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_fp_tree(in_tree, header_table, threshold, pre_fix, freq_item_list, min_height):\n",
    "    \"\"\"\n",
    "    Mine the frequent pattern itemsets from FP-tree\n",
    "    \n",
    "    :in_tree: not used\n",
    "    :threshold: The minimum support\n",
    "    :pre_fix: the frequent items set in the last recursion\n",
    "    :freq_item_list: final frequent items pattern to output\n",
    "    :min_height: The min height of conditional FP-tree to print\n",
    "    \"\"\"\n",
    "\n",
    "    items_list = [(v[0], v[1][0]) for v in sorted(header_table.items(), key=lambda p:p[1][0])] # Notice a bug p[1][0] is True, p[0] is false\n",
    "    \n",
    "    for item, count in items_list:\n",
    "        \n",
    "        new_freq_set = pre_fix.copy()\n",
    "        \n",
    "        new_freq_set.add(vocab_dict[item]) \n",
    "        #new_freq_set.add(item) # test code\n",
    "        \n",
    "        freq_item_list.append([count, new_freq_set])\n",
    "        \n",
    "        # Construct conditional pattern base\n",
    "        cond_pattern_base = find_cond_pat_base(item, header_table)\n",
    "        # print(cond_pattern_base)\n",
    "        # Build conditional FP trees\n",
    "        cond_tree, cond_header_table, _ = create_fp_tree(cond_pattern_base, threshold)\n",
    "        \n",
    "        # If FP-conditional tree exists\n",
    "        if cond_tree:\n",
    "            # Find FP-conditional tree whose height is larger than 1\n",
    "            if cond_tree.get_height() > min_height:\n",
    "                print(\"FP-cond Tree of\" + str(new_freq_set))\n",
    "                print(\"height = \" + str(cond_tree.get_height()))\n",
    "                print(\"list form of cond tree:\")\n",
    "                print(cond_tree.transform_to_list())\n",
    "                cond_tree.display()\n",
    "\n",
    "        # If header_table exists\n",
    "        if cond_header_table:\n",
    "            mine_fp_tree(cond_tree, cond_header_table, threshold, new_freq_set, freq_item_list, min_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_from_txt(topic_file, pattern_file, threshold=400, min_height=1):\n",
    "    \"\"\"\n",
    "    Mine frequent itemsets pattern from topic file\n",
    "    \n",
    "    :topic_file: (string) the path of topic file\n",
    "    :pattern_file: (string) the path to store the pattern file\n",
    "    :threshold: the minimum support\n",
    "    :min_height: the minimum height of FP-conditional tree to print\n",
    "    \"\"\"\n",
    "    items_bought = read_file(topic_file) # Read topic.txt file to initialize dataset => nested list\n",
    "    trans_dict = create_init_set(items_bought) # list => transaction dict\n",
    "    \n",
    "    fp_tree, header_table, _ = create_fp_tree(trans_dict, threshold) # Create FP-tree and header table\n",
    "\n",
    "    freq_items = [] # frequent items pattern list\n",
    "    mine_fp_tree(fp_tree, header_table, threshold, set([]), freq_items, min_height)\n",
    "    \n",
    "    # Sort freq_items by descending supported count\n",
    "    freq_items = sorted(freq_items, key=lambda p:p[0], reverse=True)\n",
    "    print(\"frequent items pattern:\")\n",
    "    print(freq_items)\n",
    "    \n",
    "    # Write the pattern in txt file\n",
    "    with open(pattern_file, \"w\") as f:\n",
    "        for count, vocabs in freq_items:\n",
    "            f.write(str(count) + \"\\t\")\n",
    "            i = 0\n",
    "            length = len(vocabs)\n",
    "            for vocab in vocabs:\n",
    "                i += 1\n",
    "                # Replace index with terms in vocab.txt\n",
    "                f.write(vocab)\n",
    "                if i < length:\n",
    "                    f.write(\" \")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# INFORM IMPRTANTLY\n",
    "# This is test code , CANNOT RUN. To run it, you need to COMMENT OUT some code\n",
    "data = [\n",
    "            ['a', 'b', 'd', 'e', 'f', 'g'],\n",
    "            ['a', 'f', 'g'],\n",
    "            ['b', 'd', 'e', 'f'],\n",
    "            ['a', 'b', 'd'],\n",
    "            ['a', 'b', 'e', 'g']\n",
    "        ]\n",
    "        \n",
    "trans_set = create_init_set(data)\n",
    "\n",
    "fp_tree, header_table, itemsets = create_fp_tree(trans_set, 3)\n",
    "\n",
    "freq_items = [] # frequent items pattern list\n",
    "mine_fp_tree(fp_tree, header_table, 3, set([]), freq_items, 1)\n",
    "    \n",
    "# Sort freq_items by descending supported count\n",
    "freq_items = sorted(freq_items, key=lambda p:p[0], reverse=True)\n",
    "print(\"frequent items pattern:\")\n",
    "print(freq_items)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the FP-conditional tree and write pattern in file\n",
    "\n",
    "According to the question description, for eache topic, we need to output to a `pattern.txt` file and we also need to find out FP-conditional tree with height more than 1. Therefore we do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing topic-0.txt\n",
      "FP-cond Tree of{'mining'}\n",
      "height = 2\n",
      "list form of cond tree:\n",
      "['Null Set    1', ['data    413']]\n",
      "   Null Set   1\n",
      "     390   413\n",
      "frequent items pattern:\n",
      "[[1288, {'data'}], [1163, {'mining'}], [1087, {'algorithm'}], [720, {'graph'}], [560, {'time'}], [528, {'pattern'}], [509, {'tree'}], [488, {'efficient'}], [416, {'rule'}], [413, {'mining', 'data'}]]\n",
      "write the pattern in pattern-0.txt\n",
      "\n",
      "processing topic-1.txt\n",
      "frequent items pattern:\n",
      "[[1488, {'learning'}], [1050, {'using'}], [819, {'model'}], [715, {'based'}], [582, {'classification'}], [488, {'feature'}], [474, {'clustering'}], [463, {'network'}]]\n",
      "write the pattern in pattern-1.txt\n",
      "\n",
      "processing topic-2.txt\n",
      "FP-cond Tree of{'retrieval'}\n",
      "height = 2\n",
      "list form of cond tree:\n",
      "['Null Set    1', ['information    475']]\n",
      "   Null Set   1\n",
      "     190   475\n",
      "frequent items pattern:\n",
      "[[1226, {'web'}], [1211, {'information'}], [1114, {'retrieval'}], [863, {'based'}], [757, {'system'}], [707, {'search'}], [564, {'document'}], [490, {'language'}], [475, {'information', 'retrieval'}], [421, {'model'}], [414, {'semantic'}]]\n",
      "write the pattern in pattern-2.txt\n",
      "\n",
      "processing topic-3.txt\n",
      "frequent items pattern:\n",
      "[[1074, {'database'}], [928, {'system'}], [743, {'knowledge'}], [558, {'learning'}], [514, {'data'}], [506, {'logic'}], [493, {'reasoning'}], [446, {'model'}], [426, {'constraint'}]]\n",
      "write the pattern in pattern-3.txt\n",
      "\n",
      "processing topic-4.txt\n",
      "frequent items pattern:\n",
      "[[1713, {'query'}], [1163, {'database'}], [1040, {'data'}], [767, {'system'}], [637, {'processing'}], [567, {'distributed'}], [528, {'object'}], [508, {'efficient'}], [458, {'xml'}]]\n",
      "write the pattern in pattern-4.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all topic files\n",
    "for i in range(5):\n",
    "    topic_file = \"topic-\" + str(i) + \".txt\"\n",
    "    pattern_file = \"pattern-\" + str(i) + \".txt\"\n",
    "    print(\"processing \" + topic_file)\n",
    "    mine_from_txt(topic_file, pattern_file, threshold=400, min_height=1)\n",
    "    print(\"write the pattern in \" + pattern_file)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
