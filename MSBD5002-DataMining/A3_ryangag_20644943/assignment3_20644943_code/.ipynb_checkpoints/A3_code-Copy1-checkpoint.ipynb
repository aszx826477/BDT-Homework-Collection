{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Utils Functions Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import useful packages\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_k_folders(dataset, k):\n",
    "    \"\"\"\n",
    "    Generate K-folders\n",
    "    \n",
    "    Input: dataset and k\n",
    "    Output: a list contains k dictionary, each dictionary contains training set, validation set and testing set\n",
    "    \"\"\"\n",
    "    \n",
    "    x = dataset[\"x_train\"]\n",
    "    y = dataset[\"y_train\"]\n",
    "    x_test = dataset[\"x_test\"]\n",
    "    y_test = dataset[\"y_test\"]\n",
    "    \n",
    "    k_folders = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        if i < (k-1):     \n",
    "            a = i*int(x.shape[0]/k)\n",
    "            b = (i+1)*int(x.shape[0]/k)\n",
    "            k_folders.append({\n",
    "                \"x_train\": torch.cat((x[:a], x[b:]), dim=0),\n",
    "                \"y_train\": torch.cat((y[:a], y[b:])),\n",
    "                \"x_val\": x[a:b],\n",
    "                \"y_val\": y[a:b],\n",
    "                \"x_test\": x_test,\n",
    "                \"y_test\": y_test\n",
    "            })\n",
    "        else:\n",
    "            a = i*int(x.shape[0]/k)\n",
    "            k_folders.append({\n",
    "                \"x_train\": x[:a],\n",
    "                \"y_train\": y[:a],\n",
    "                \"x_val\": x[a:],\n",
    "                \"y_val\": y[a:],\n",
    "                \"x_test\": x_test,\n",
    "                \"y_test\": y_test\n",
    "            })\n",
    "            \n",
    "    return k_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_dataset(dataset):\n",
    "    \"\"\"\n",
    "    upzip dataset\n",
    "    \"\"\"\n",
    "    x_train = dataset[\"x_train\"]\n",
    "    y_train = dataset[\"y_train\"]\n",
    "    x_val = dataset[\"x_val\"]\n",
    "    y_val = dataset[\"y_val\"]\n",
    "    x_test = dataset[\"x_test\"]\n",
    "    y_test = dataset[\"y_test\"]\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_mat_evaluate(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate the model performance by confusion matrix\n",
    "    \n",
    "    Input: y_predict and the truth label y_test\n",
    "    Output: accuracy, precision, recall, f1\n",
    "    \"\"\" \n",
    "    \n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred, average=\"macro\")\n",
    "    recall = metrics.recall_score(y_test, y_pred, average=\"macro\")\n",
    "    f1 = metrics.f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bi_data(dataset):\n",
    "    \"\"\"\n",
    "    Read binary-class dataset\n",
    "    \n",
    "    Input: (numpy.array) dataset\n",
    "    Output: a list consists of x_train, y_train, x_test and y_test\n",
    "    \"\"\"\n",
    "    x_train = torch.from_numpy(dataset['train_X']).type(torch.FloatTensor).cuda()\n",
    "    y_train = torch.from_numpy(dataset['train_Y']).type(torch.FloatTensor).cuda() \n",
    "    x_test = torch.from_numpy(dataset['test_X']).type(torch.FloatTensor).cuda()\n",
    "    y_test = torch.from_numpy(dataset['test_Y']).type(torch.FloatTensor).cuda()\n",
    "    \n",
    "    dataset = {\n",
    "        'x_train' : x_train,\n",
    "        'y_train' : y_train,\n",
    "        'x_test' : x_test,\n",
    "        'y_test' : y_test\n",
    "    }\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_multi_data():\n",
    "    \"\"\"\n",
    "    Read multi-class dataset\n",
    "    \n",
    "    Output: a list consists of x_train, y_train, x_test and y_test\n",
    "    \"\"\"\n",
    "    x_train = torch.from_numpy(loadmat(\"datasets/multi-class/train_images.mat\")[\"train_images\"]).type(torch.FloatTensor).cuda()\n",
    "    y_train = torch.from_numpy(loadmat(\"datasets/multi-class/train_labels.mat\")[\"train_labels\"]).type(torch.LongTensor).cuda()\n",
    "    y_train = y_train.t().squeeze(dim=-1)\n",
    "   \n",
    "\n",
    "    x_test = torch.from_numpy(loadmat(\"datasets/multi-class/test_images.mat\")[\"test_images\"]).type(torch.FloatTensor).cuda()\n",
    "    y_test = torch.from_numpy(loadmat(\"datasets/multi-class/test_labels.mat\")[\"test_labels\"]).type(torch.LongTensor).cuda()\n",
    "    y_test = y_test.t().squeeze(dim=-1)\n",
    "    \n",
    "    dataset = {\n",
    "        'x_train' : x_train,\n",
    "        'y_train' : y_train,\n",
    "        'x_test' : x_test,\n",
    "        'y_test' : y_test\n",
    "    }\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Pytorch NN on Five Classifcation Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "breast_cancer_data = np.load(\"datasets/bi-class/breast-cancer.npz\")\n",
    "diabetes_data = np.load(\"datasets/bi-class/diabetes.npz\")\n",
    "digit_data = np.load(\"datasets/bi-class/digit.npz\")\n",
    "iris_data = np.load(\"datasets/bi-class/iris.npz\")\n",
    "wine_data = np.load(\"datasets/bi-class/wine.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bi_nn_model(dataset, H_list, device, learning_rate=1e-2, iteration=5000):\n",
    "    \n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = unzip_dataset(dataset)\n",
    "\n",
    "    N = x_train.shape[0]\n",
    "    D_in = x_train.shape[1]\n",
    "    D_out = 1\n",
    "    \n",
    "    # Binary Cross Entropy Loss\n",
    "    loss_fn = torch.nn.BCELoss() \n",
    "    \n",
    "    # The result table\n",
    "    # Each row preserves the related result of corresponding H\n",
    "    # 8 means we have 8 items to save => [H, best_accuracy_val, accuracy_test, auc, precision, recall, f1, training_time]\n",
    "    res_table = np.zeros((len(H_list),8))\n",
    "    res_table_ind = 0\n",
    "    \n",
    "    for H in H_list:\n",
    "        \n",
    "        model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(D_in, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, D_out),\n",
    "            torch.nn.Sigmoid()\n",
    "        ).to(device)\n",
    "        \n",
    "        loss_history = []\n",
    "        accuracy_val = []\n",
    "        correct = 0.0\n",
    "        best_accuracy_val = 0.0\n",
    "        best_iteration = 0.0\n",
    "        best_model = None\n",
    "        \n",
    "        # SGD optimizer\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        for t in range(iteration):\n",
    "            y_train_pred = model(x_train).squeeze(dim=-1) # squeeze 2D of shape(x,1) to 1D of shape(x,)\n",
    "            loss = loss_fn(y_train_pred, y_train)\n",
    "            loss_history.append(loss.item())\n",
    "            \n",
    "            # Transfer the prediction result from probability to [0,1] label\n",
    "            y_val_pred = model(x_val).squeeze(dim=-1)\n",
    "            y_val_pred[y_val_pred >= 0.5] = 1.0\n",
    "            y_val_pred[y_val_pred < 0.5] = 0.0\n",
    "            \n",
    "            # Calculate accuracy on the validation set\n",
    "            correct = (y_val_pred == y_val).sum().item()\n",
    "            accuracy = correct / y_val.shape[0]\n",
    "            accuracy_val.append(accuracy)\n",
    "            \n",
    "            # Save the best model and best accuracy\n",
    "            if accuracy > best_accuracy_val:\n",
    "                best_accuracy_val = accuracy\n",
    "                best_iteration = t\n",
    "                best_model = model\n",
    "            \n",
    "            if t % 1000 == 0:   \n",
    "                print(\"iteration: %s/%s\" % (t, iteration))\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Compute training time\n",
    "        t1 = time.time()\n",
    "        print(\"training time = %s(s)\" % (t1 - t0))\n",
    "        \n",
    "        # Use the best model to predict on testing dataset\n",
    "        y_test_pred = best_model(x_test).squeeze(dim=-1)\n",
    "\n",
    "        # Detach data from the graph and transfer to numpy array\n",
    "        yt = y_test.cpu().detach().numpy()\n",
    "        yp = y_test_pred.cpu().detach().numpy()\n",
    "        \n",
    "        # Compute AUC (Area Under ROC Curve)\n",
    "        auc = metrics.roc_auc_score(yt, yp)\n",
    "        \n",
    "        # Compute accuracy, precision, recall and f1\n",
    "        yp[yp >= 0.5] = 1.0 # probalility >= 0.5 atached to positive label\n",
    "        yp[yp < 0.5] = 0.0 # probability < 0.5 atached to negative label\n",
    "        accuracy_test, precision, recall, f1 = confusion_mat_evaluate(yt, yp)\n",
    "        \n",
    "        # Save all the results in res_table\n",
    "        res_table[res_table_ind] += np.array([H, best_accuracy_val, accuracy_test, auc, precision, recall, f1, t1-t0])\n",
    "        res_table_ind += 1\n",
    "        \n",
    "        # Output the result\n",
    "        print(\"The best model: iteration = %s\" % (best_iteration))\n",
    "        print(\"On validation dataset: accuracy = %s\" % (best_accuracy_val))\n",
    "        print(\"On testing dataset: accuracy = %s, auc = %s, precision = %s, recall = %s, f1 = %s\" \\\n",
    "              % (accuracy_test, auc, precision, recall, f1))\n",
    "\n",
    "        # Plot the loss curve and varlidation accuracy curve\n",
    "        fig = plt.figure(figsize=(15,4))\n",
    "        ax1 = plt.subplot(1,2,1)\n",
    "        ax2 = plt.subplot(1,2,2)\n",
    "        plt.sca(ax1)\n",
    "        plt.title(\"H = %s\" % H)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.plot(range(iteration), loss_history)\n",
    "        plt.sca(ax2)\n",
    "        plt.title(\"H = %s\" % H)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(\"accuracy_val\")\n",
    "        plt.plot(range(iteration), accuracy_val)\n",
    "        plt.show()\n",
    "\n",
    "    # Return the result tables\n",
    "    return res_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def framework_run(data, device, k, H_list, lr, iteration):\n",
    "    # Generate K-folders\n",
    "    k_folders = generate_k_folders(read_bi_data(breast_cancer_data), k)\n",
    "\n",
    "    # The result table\n",
    "    # Each row preserves the related result of corresponding H\n",
    "    # 8 means we have 8 items to save => [H, best_accuracy_val, accuracy_test, auc, precision, recall, f1, training_time]\n",
    "    res_table = np.zeros((len(H_list), 8))\n",
    "    \n",
    "    # Traversal all folders dataset\n",
    "    for index in range(k):\n",
    "        print(\"K-Folder index = %s\" % index)\n",
    "        # Sum up the result table\n",
    "        res_table += train_bi_nn_model(k_folders[index], H_list, device, lr, iteration)\n",
    "\n",
    "    # Return the average table\n",
    "    return np.round(res_table / k, decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing part\n",
    "\n",
    "If this part running successfully, it means that all functions works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "framework_run(breast_cancer_data, device, k=5, H_list=[5,6], lr=5e-3, iteration=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on five binary classification data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# breast_data features = 10 => H* = 3, lr = 1e-2\n",
    "framework_run(breast_cancer_data, device, k=5, H_list=[1,2,3,4,5,6,7], lr=1e-2, iteration=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|H_breastNN|Val_Accuracy|Test_Accuracy|AUC|Precision|Recall| F1 |Training Time|\n",
    "|---------|---|------------|-------------|---------|------|----|-------------|\n",
    "| 1   |  0.892|  0.901|  0.892|  0.838|  0.866|  0.848|  9.782(s)|\n",
    "| 2   |  0.973|  0.963|  0.997|  0.962|  0.957|  0.96 |  9.643(s)|\n",
    "| 3   |  0.973|  0.969|  0.997|  0.967|  0.966|  0.966|  9.4(s)  |\n",
    "| 4   |  0.965|  0.966|  0.998|  0.964|  0.962|  0.963| 10.327(s)|\n",
    "| 5   |  0.963|  0.968|  0.997|  0.965|  0.964|  0.965| 11.125(s)|\n",
    "| 6   |  0.965|  0.968|  0.997|  0.965|  0.964|  0.965| 11.011(s)|\n",
    "| 7   |  0.973|  0.968|  0.998|  0.965|  0.964|  0.965| 10.401(s)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# digit_data features = 64 => H* = 10, lr = 2e-3\n",
    "framework_run(digit_data, device, k=5, H_list=[5,6,7,8,9,10], lr=2e-3, iteration=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|H_digitNN|Val_Accuracy|Test_Accuracy|AUC|Precision|Recall| F1 |Training Time|\n",
    "|---------|---|------------|-------------|---------|------|----|-------------|\n",
    "| 5   |  0.914|  0.859|  0.987|  0.818|  0.8  |  0.793|  9.569(s)|\n",
    "| 6   |  0.947|  0.931|  0.998|  0.951|  0.903|  0.92 |  9.327(s)|\n",
    "| 7   |  0.927|  0.935|  0.997|  0.956|  0.908|  0.924|  9.11 (s)|\n",
    "| 8   |  0.941|  0.929|  0.998|  0.951|  0.9  |  0.918| 11.058(s)|\n",
    "| 9   |  0.919|  0.922|  0.997|  0.948|  0.89 |  0.907| 11.388(s)|\n",
    "|10   |  0.956|  0.94 |  0.997|  0.958|  0.915|  0.93 | 10.615(s)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# diabetes_data features = 8 => H* = 4, lr = 5e-2\n",
    "framework_run(diabetes_data, device, k=5, H_list=[1,2,3,4,5,6,7,8], lr=5e-2, iteration=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|H_diabetesNN|Val_Accuracy|Test_Accuracy|AUC|Precision|Recall| F1 |Training Time|\n",
    "|---------|---|------------|-------------|---------|------|----|-------------|\n",
    "| 1   |  0.926|  0.9  |  0.897|  0.833|  0.867|  0.846|  9.412(s)|\n",
    "| 2   |  0.973|  0.965|  0.997|  0.961|  0.961|  0.961|  9.183(s)|\n",
    "| 3   |  0.967|  0.968|  0.997|  0.964|  0.966|  0.965|  9.584(s)|\n",
    "| 4   |  0.971|  0.969|  0.997|  0.966|  0.967|  0.966|  9.897(s)|\n",
    "| 5   |  0.973|  0.968|  0.997|  0.965|  0.965|  0.965|  9.909(s)|\n",
    "| 6   |  0.976|  0.965|  0.997|  0.961|  0.962|  0.961| 10.188(s)|\n",
    "| 7   |  0.971|  0.966|  0.997|  0.963|  0.963|  0.963| 10.127(s)|\n",
    "| 8   |  0.971|  0.968|  0.997|  0.963|  0.966|  0.965|  9.489(s)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iris_data features = 4 => H* = 3, lr = 1e-2\n",
    "framework_run(iris_data, device, k=5, H_list=[1,2,3,4], lr=1e-2, iteration=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|H_irisNN|Val_Accuracy|Test_Accuracy|AUC|Precision|Recall| F1 |Training Time|\n",
    "|---------|---|------------|-------------|---------|------|----|-------------|\n",
    "|1   | 0.886| 0.903| 0.891| 0.837| 0.87 | 0.85 | 9.465(s)|\n",
    "|2   | 0.962| 0.965| 0.997| 0.963| 0.959| 0.961| 9.192(s)|\n",
    "|3   | 0.963| 0.966| 0.997| 0.964| 0.962| 0.963| 9.107(s)|\n",
    "|4   | 0.965| 0.963| 0.998| 0.962| 0.957| 0.96 | 9.198(s)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wine_data 13 features => H* = 6, lr = 1e-3 \n",
    "framework_run(wine_data, device, k=5, H_list=list(range(1,11)), lr=1e-3, iteration=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|H_wineNN|Val_Accuracy|Test_Accuracy|AUC|Precision|Recall| F1 |Training Time|\n",
    "|---------|---|------------|-------------|---------|------|----|-------------|\n",
    "| 1   |  0.715|  0.709|  0.827|  0.643|  0.674|  0.6  |  9.564(s)|\n",
    "| 2   |  0.822|  0.781|  0.979|  0.678|  0.691|  0.66 |  9.17(s) |\n",
    "| 3   |  0.758|  0.744|  0.869|  0.662|  0.638|  0.601|  9.814(s)|\n",
    "| 4   |  0.864|  0.841|  0.979|  0.901|  0.778|  0.791| 10.202(s)|\n",
    "| 5   |  0.734|  0.751|  0.929|  0.668|  0.648|  0.604|  9.864(s)|\n",
    "| 6   |  0.861|  0.834|  0.997|  0.902|  0.765|  0.773|  9.523(s)|\n",
    "| 7   |  0.824|  0.807|  0.964|  0.787|  0.728|  0.723|  9.795(s)|\n",
    "| 8   |  0.87 |  0.815|  0.996|  0.892|  0.737|  0.748|  9.733(s)|\n",
    "| 9   |  0.83 |  0.803|  0.997|  0.886|  0.721|  0.73 |  9.45(s) |\n",
    "|10   |  0.813|  0.799|  0.997|  0.885|  0.715|  0.716| 10.286(s)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Pytorch NN for Multi-class Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multi_nn_model(dataset, L1_list, L2_list, device, learning_rate=1e-2, iteration=5000):\n",
    "    \n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = unzip_dataset(dataset)\n",
    "    \n",
    "    # Cross Entropy Loss: combines nn.LogSoftmax() and nn.NLLLoss()\n",
    "    loss_fn = torch.nn.CrossEntropyLoss() \n",
    "    \n",
    "    # The result table\n",
    "    # Each row preserves the related result of corresponding combination of L1 and L2\n",
    "    # 5 means we have 5 items to save => [L1, L2, best_accuracy_val, accuracy_test, training_time]\n",
    "    res_table = np.zeros((len(L1_list)*len(L2_list),5))\n",
    "    res_table_ind = 0\n",
    "    \n",
    "    for L1 in L1_list:\n",
    "        for L2 in L2_list:\n",
    "            t0 = time.time()\n",
    "            model = torch.nn.Sequential(\n",
    "                torch.nn.Linear(784, L1), # input dimension = 784, hidden layer1 = L1\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(L1, L2), # hidden layer2 = L2\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(L2, 10), # output probability on 10 classes\n",
    "            ).to(device)\n",
    "        \n",
    "            loss_history = []\n",
    "            accuracy_val = []\n",
    "            correct = 0.0\n",
    "            best_accuracy_val = 0.0\n",
    "            best_model = None\n",
    "            \n",
    "            # SGD optimizer\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "            \n",
    "            for t in range(iteration):\n",
    "                y_train_pred = model(x_train).squeeze(dim=-1) # squeeze 2D of shape(x,1) to 1D of shape(x,)\n",
    "                loss = loss_fn(y_train_pred, y_train)\n",
    "                loss_history.append(loss.item())\n",
    "     \n",
    "                # Choose the max possibility index as the prediction class\n",
    "                y_val_pred = model(x_val)\n",
    "                y_val_pred_label = torch.argmax(y_val_pred, dim=1)\n",
    "                \n",
    "                # Calculate accuracy on the validation set\n",
    "                correct = (y_val_pred_label == y_val).sum().item()\n",
    "                accuracy = correct / y_val.shape[0]\n",
    "                accuracy_val.append(accuracy)\n",
    "            \n",
    "                # Save the best model and best accuracy\n",
    "                if accuracy > best_accuracy_val:\n",
    "                    best_accuracy_val = accuracy\n",
    "                    best_model = model\n",
    "            \n",
    "                if t % 200 == 0:   \n",
    "                    print(\"iteration: %s/%s\" % (t, iteration))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "            # Compute training time\n",
    "            t1 = time.time()\n",
    "            print(\"training time = %s(s)\" % (t1 - t0))\n",
    "        \n",
    "            # Use the best model to predict on testing dataset\n",
    "            y_test_pred = best_model(x_test)\n",
    "            y_test_pred_label = torch.argmax(y_test_pred, dim=1)\n",
    "                \n",
    "            # Calculate accuracy on the testing dataset\n",
    "            correct = (y_test_pred_label == y_test).sum().item()\n",
    "            accuracy_test = correct / y_val.shape[0]\n",
    "            \n",
    "            # Save in result table\n",
    "            res_table[res_table_ind] += np.array([L1, L2, best_accuracy_val, accuracy_test, t1-t0])\n",
    "            res_table_ind += 1\n",
    "            \n",
    "            print(\"best_accuracy_val = %s, accuracy_test = %s\" % (best_accuracy_val, accuracy_test))\n",
    "\n",
    "            \n",
    "            # Plot the loss curve and varlidation accuracy curve\n",
    "            fig = plt.figure(figsize=(15,4))\n",
    "            ax1 = plt.subplot(1,2,1)\n",
    "            ax2 = plt.subplot(1,2,2)\n",
    "            plt.sca(ax1)\n",
    "            plt.title(\"L1 = %s, L2 = %s\" % (L1, L2))\n",
    "            plt.xlabel(\"iteration\")\n",
    "            plt.ylabel(\"loss\")\n",
    "            plt.plot(range(iteration), loss_history)\n",
    "            plt.sca(ax2)\n",
    "            plt.title(\"L1 = %s, L2 = %s\" % (L1, L2))\n",
    "            plt.xlabel(\"iteration\")\n",
    "            plt.ylabel(\"accuracy_val\")\n",
    "            plt.plot(range(iteration), accuracy_val)\n",
    "            fig.tight_layout(pad=0.4, w_pad=3.0, h_pad=3.0)            \n",
    "            plt.show()\n",
    "            \n",
    "    # Return the result tables\n",
    "    return res_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate 5-folders\n",
    "k_folders = generate_k_folders(read_multi_data(), 5)\n",
    "\n",
    "# The trying parameters of L1 and L2\n",
    "L1_list = [50, 75, 100]\n",
    "L2_list = [10, 15, 20]\n",
    "\n",
    "# The result table\n",
    "# Each row preserves the related result of corresponding combination of L1 and L2\n",
    "# 5 means we have 5 items to save => [L1, L2, best_accuracy_val, accuracy_test, training_time]\n",
    "res_table = np.zeros((len(L1_list)*len(L2_list),5))\n",
    "\n",
    "for index in range(5):\n",
    "    print(\"K-Folder index = %s\" % index)\n",
    "    # Sum up the result table\n",
    "    res_table += train_multi_nn_model(k_folders[index], L1_list, L2_list, device, learning_rate=1e-4, iteration=1000)\n",
    "\n",
    "# Compute the average table\n",
    "np.round(res_table / 5, decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|L1|L2|Val Accuracy|Test Accuracy|Training Time|\n",
    "|--|--|------------|-------------|-------------|\n",
    "| 50   |  10   |   0.454|   0.221|  11.726(s)|\n",
    "| 50   |  15   |   0.601|   0.298|  11.976(s)|\n",
    "| 50   |  20   |   0.704|   0.352|  12.125(s)|\n",
    "| 75   |  10   |   0.413|   0.208|  12.466(s)|\n",
    "| 75   |  15   |   0.615|   0.304|  12.493(s)|\n",
    "| 75   |  20   |   0.691|   0.346|  12.867(s)|\n",
    "|100   |  10   |   0.529|   0.256|  13.829(s)|\n",
    "|100   |  15   |   0.585|   0.282|  13.956(s)|\n",
    "|100   |  20   |   0.651|   0.32 |  13.946(s)|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
